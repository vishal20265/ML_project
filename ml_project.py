# -*- coding: utf-8 -*-
"""ML_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eEkrYYQVA8S8Dx3I4utQnOc0u167HWT4
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
df = pd.read_csv('/content/drive/MyDrive/heart_disease_health_indicators_BRFSS2015.csv')
df.head()

"""To chech for null values"""

df.info()

df.isnull().sum().sum()

"""there is no null values in dataset

Columns of the dataset
"""

df.columns

df.describe()

"""Observations-

Mean value is less than median value of each column represented by 50%(50th percentile) in index column.
Notably large differnece in 75th %tile and max values of predictors "Stroke","Diabetes","MentHlth", "PhysHlth",
In "BMI","MentHlth", "PhysHlth" you can see the maxmimum value is completely/significantly  outlier to the distribution of the column.

TO counter this we can apply normalization to "BMI","MentHlth", "PhysHlth" like min-max , standard-scalar. In this we are applying min-max because min-max difference is significantly large.
"""

from sklearn.preprocessing import LabelEncoder, MinMaxScaler
for feature in ['BMI', 'MentHlth', 'PhysHlth']: 
    df[feature] = df[feature].astype('int64')
    df[feature] = MinMaxScaler(feature_range=(0, 1)).fit_transform(df[[feature]])

df.describe()

df.HeartDiseaseorAttack.unique()

"""Target variable/Dependent variable is discrete and categorical in nature.
"HeartDiseaseorAttack" score is between 0 or 1;where 1 having a heart disease/attack and 0 not having a heart disease/attack.
"""

df.HeartDiseaseorAttack.value_counts()

"""This tells us how many people have heart disease/attack and how many do not.
Many people do not have heart disease/attack compared to who have.

Pairplots are also a great way to immediately see the correlations between all variables.
"""

import seaborn as sns

sns.pairplot(df.sample(1000), hue='HeartDiseaseorAttack', dropna=True);

features = list(df.columns)
features.remove('HeartDiseaseorAttack')
target = 'HeartDiseaseorAttack'

"""BOX-PLOTS"""

for i in features:
  print(i)
  sns.boxplot(x=i, data=df)
  plt.show()

"""Heat - Map for co relation"""

import seaborn as sns
x = df[features].sample(1000)
sns.heatmap(x.corr(method='pearson'), annot=False, fmt='.1f')

"""Light shades represents positive correlation while Darker shades represents negative correlation. If we set annot=True, we'll get values by which features are correlated to each other in grid-cells"""

#Quality correlation matrix
k = 22 #number of variables for heatmap
cols = df.corr().nlargest(k, 'Smoker')['Smoker'].index
cm = df[cols].corr()
plt.figure(figsize=(20,12))
sns.heatmap(cm, annot=True, cmap = 'viridis')

"""Here we can infer that "PhysHlth" has strong positive correlation with "GenHlth" whereas it has strong negative correlation with "income". "AnyHlthCare" and "NoDocBcCost" has almost no correlation with "HeartDiseaseorAttack" Since correlation is zero we can infer there is no linear relationship between these two predictors.However it is safe to drop these features in case you're applying Linear Regression model to the dataset."""

plt.figure(figsize=(15,50))
for i,column in enumerate(features[1:]):
    plt.subplot(len(features), 2, i+1)
    plt.suptitle("Each feature VS HeartAttack", fontsize=20, x=0.5, y=1)
    sns.countplot(data=df, x=column, hue='HeartDiseaseorAttack')
    plt.title(f"{column}")
    plt.tight_layout()

plt.figure(figsize=(15,40))
for i,column in enumerate(list(df.columns)):
    plt.subplot(len(list(df.columns)), 2, i+1)
    plt.suptitle("Plot Value Proportion", fontsize=20, x=0.5, y=1)
    plt.pie(x=df[column].value_counts(), labels=df[column].unique(), autopct='%.0f%%')
    plt.title(f"{column}")
    plt.tight_layout()

X = df.drop(columns='HeartDiseaseorAttack')
Y = df['HeartDiseaseorAttack']

"""Feature selection out of 21 features , which are redudant with the help of PCA and calculating the accuracies for diff output dimension of pca"""

from pandas.io.stata import PossiblePrecisionLoss
from sklearn.decomposition import PCA
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn import decomposition
for i in range(8,16,2):
  pca = PCA(n_components=i)
  pca_data = pca.fit_transform(X)
  pca_pd = pd.DataFrame(pca_data)
  X_train, X_test, y_train, y_test = train_test_split(pca_data, Y, test_size=0.2, random_state=0)
  gnb = GaussianNB()
  y_predict = gnb.fit(X_train, y_train).predict(X_test)
  # con_mat = cm((y_test),(y_predict))
  
  recall = recall_score(y_test,y_predict,average='weighted')
  print("The recall for","no of components",i,"is",recall)

  f1 = f1_score(y_test,y_predict,average='weighted')
  print("The f1 for","no of components",i,"is",f1)

  acc = accuracy_score(y_test,y_predict)
  print("The accuracy for","no of components",i,"is",acc)

  precisin = precision_score(y_test,y_predict,average='weighted')
  print("The precision for","no of components",i,"is",precisin)
  print("")
  print("")

"""Now our data is ready to train a model"""

pca = PCA(n_components=8)
pca_data = pca.fit_transform(X)
fi_pd = pd.DataFrame(pca_data)
X_train, X_test, y_train, y_test = train_test_split(fi_pd, Y, test_size=0.2, random_state=0)

"""Gaussian Naive Bayes"""

gnb = GaussianNB()
y_predict = gnb.fit(X_train, y_train).predict(X_test)
recall = recall_score(y_test,y_predict,average='weighted')
print("The recall ","is",recall)
print()
f1 = f1_score(y_test,y_predict,average='weighted')
print("The f1 ","is",f1)
print()
acc = accuracy_score(y_test,y_predict)
print("The accuracy ","is",acc)
print()
precisin = precision_score(y_test,y_predict,average='weighted')
print("The precision","is",precisin)
print()

"""logistic regression"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000,tol = 100)
model.fit(X_train,y_train)
ypred_lg = model.predict(X_test)
recall = recall_score(y_test,ypred_lg,average='weighted')
print("The recall ","is",recall)
print()
f1 = f1_score(y_test,ypred_lg,average='weighted')
print("The f1 ","is",f1)
print()
acc = accuracy_score(y_test,ypred_lg)
print("The accuracy ","is",acc)
print()
precisin = precision_score(y_test,ypred_lg,average='weighted')
print("The precision","is",precisin)
print()

"""Decision trees classifier"""

from sklearn.metrics import roc_curve, auc

from sklearn.tree import DecisionTreeClassifier

gini_decision_model = DecisionTreeClassifier(criterion='gini')
gini_decision_model.fit(X_train, y_train)
gini_pred = gini_decision_model.predict(X_test)
score = accuracy_score(gini_pred, y_test)
print(f"Accuracy with gini comes out to be {score}")
print("Recall score: ", recall_score(gini_pred, y_test, average="weighted"))
print("f1 score: ", f1_score(gini_pred, y_test, average="weighted"))
print("Recall score: ", precision_score(gini_pred, y_test, average="weighted"))
fpr, tpr, thresh = roc_curve(gini_pred, y_test)
plt.plot(fpr, tpr, label=auc(fpr, tpr))
plt.fill_between(fpr, tpr, alpha=0.6)
plt.legend()
plt.show()

from sklearn.tree import DecisionTreeClassifier
gini_decision_model = DecisionTreeClassifier(criterion='entropy')
gini_decision_model.fit(X_train, y_train)
gini_pred = gini_decision_model.predict(X_test)
score = accuracy_score(gini_pred, y_test)
print(f"Accuracy with gini comes out to be {score}")
print("Recall: ", recall_score(gini_pred, y_test, average="weighted"))
print("f1: ", f1_score(gini_pred, y_test, average="weighted"))
print("precision: ", precision_score(gini_pred, y_test, average="weighted"))
fpr, tpr, thresh = roc_curve(gini_pred, y_test)
plt.plot(fpr, tpr, label=auc(fpr, tpr))
plt.fill_between(fpr, tpr, alpha=0.6)
plt.legend()
plt.show()

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=20, criterion="entropy")
model.fit(X_train, y_train)
pred = model.predict(X_test)
score = accuracy_score(pred, y_test)
print("Random Forest Entropy got accuracy: {}".format(score))
fpr, tpr, thresh = roc_curve(pred, y_test)
plt.plot(fpr, tpr, label=auc(fpr, tpr))
plt.fill_between(fpr, tpr, alpha=0.6)
plt.legend()
plt.show()

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=20, criterion="gini")
model.fit(X_train, y_train)
pred = model.predict(X_test)
score = accuracy_score(pred, y_test)
print("Using gini random forest we get an accuracy score of: ", score)
fpr, tpr, thresh = roc_curve(pred, y_test)
plt.plot(fpr, tpr, label=auc(fpr, tpr))
plt.fill_between(fpr, tpr, alpha=0.6)
plt.legend()
plt.show()

from sklearn.ensemble import AdaBoostClassifier
baseTree = DecisionTreeClassifier(criterion="entropy")
model = AdaBoostClassifier(baseTree)
model.fit(X_train, y_train)
pred = model.predict(X_test)
score = accuracy_score(pred, y_test)
print("Using entropy ada boost classifier we get an accuracy of :", score)
fpr, tpr, thresh = roc_curve(pred, y_test)
plt.plot(fpr, tpr)

from sklearn.svm import SVC
model = SVC()
model.fit(X_train, y_train)
pred = model.predict(X_test)
score = accuracy_score(pred, y_test)
print("SVM got an accuracy of: ", score)

from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt
hiddenLayers = [128, 64, 32, 8, 1]
model = MLPClassifier(hiddenLayers, max_iter=100)
model.fit(X_train, y_train)
pred = model.predict(X_test)
score = accuracy_score(pred, y_test)
print("MLP Classifier got a accuracy score: ", score)

plt.plot(model.loss_curve_, label="training loss")
model = MLPClassifier(hiddenLayers, max_iter=100)
model.fit(X_test, y_test)
plt.plot(model.loss_curve_, label="testing loss")
plt.legend()
plt.xlabel("iterations")
plt.ylabel("loss")
plt.show()

pip install neupy

from neupy import algorithms
std = [0.01]
for s in std:
  print("Standard deviation set to: ", s)
  model = algorithms.PNN(std=s)
  model.fit(X_train, y_train)
  pred = model.predict(X_test)
  score = accuracy_score(pred, y_test)
  print("PNN got accuracy: ", score)

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
score = accuracy_score(y_pred, y_test)
print("K_Neighbors Classifier got an accuracy of :", score)

print("recall: ", recall_score(y_pred, y_test, average="weighted"))
print("f1: ", f1_score(y_pred, y_test, average="weighted"))
print("precision: ", precision_score(y_pred, y_test, average="weighted"))

from sklearn.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(GaussianNB(), n_features_to_select=5, cv=5, n_jobs=-1)
sfs1.fit(X_train, y_train)

sfs1.transform(X_train)

model = GaussianNB()
model.fit(sfs1.transform(X_train), y_train)
predict = model.predict(sfs1.transform(X_test))
score = accuracy_score(predict, y_test)
print("The score now is: ", score)
print("recall: ", recall_score(predict, y_test, average="weighted"))
print("f1: ", f1_score(predict, y_test, average="weighted"))
print("precision: ", precision_score(predict, y_test, average="weighted"))
predict = model.predict(sfs1.transform(X_train))
score = accuracy_score(predict, y_train)
print("Training score is:", score)

model = MLPClassifier(hiddenLayers, max_iter=100)
model.fit(sfs1.transform(X_train), y_train)
predict = model.predict(sfs1.transform(X_test))
score = accuracy_score(predict, y_test)
print("MLP score after sequential forward selection: ", score)
print("recall: ", recall_score(predict, y_test, average="weighted"))
print("f1: ", f1_score(predict, y_test, average="weighted"))
print("precision: ", precision_score(predict, y_test, average="weighted"))
predict = model.predict(sfs1.transform(X_train))
score= accuracy_score(predict, y_train)
print("Training score: ", score)

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=8)
model.fit(sfs1.transform(X_train), y_train)
y_pred = model.predict(sfs1.transform(X_test))
score = accuracy_score(y_pred, y_test)
print("K_Neighbors Classifier got an accuracy of :", score)
print("recall: ", recall_score(y_pred, y_test, average="weighted"))
print("f1: ", f1_score(y_pred, y_test, average="weighted"))
print("precision: ", precision_score(y_pred, y_test, average="weighted"))
y_pred = model.predict(sfs1.transform(X_train))
score = accuracy_score(y_pred, y_train)
print("Training set accuracy: ", score)

def newmodel():
  modelA = GaussianNB()
  modelA.fit(sfs1.transform(X_train), y_train)
  y_predA=modelA.predict(sfs1.transform(X_test))
  modelB = MLPClassifier(hiddenLayers, max_iter=100)
  modelB.fit(sfs1.transform(X_train), y_train)
  y_predB=modelB.predict(sfs1.transform(X_test))
  modelC = KNeighborsClassifier(n_neighbors=16)
  modelC.fit(sfs1.transform(X_train), y_train)
  y_predC=modelC.predict(sfs1.transform(X_test))
  pred_final=[]
  for i in range(len(y_predA)):
    l=[]
    l.append(y_predA[i])
    l.append(y_predB[i])
    l.append(y_predC[i])
    count_0=0;
    count_1=0;
    for j in l:
      if(j==0.0):
        count_0+=1
      else:
        count_1+=1
    if(count_1>=count_0):
      pred_final.append(1.0)
    else:
      pred_final.append(0.0)
  score = accuracy_score(pred_final, y_test)
  print("Score of the ownbuilt model",score)

newmodel()

